{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"name":"d1pm.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"MCKRVeG7Rz1D"},"source":["# Week 4: Day 1 PM // EDA"]},{"cell_type":"markdown","metadata":{"id":"wZCZ_2QOUlUz"},"source":["## Basic Web Scraping\n","\n","Scrapy can be installed either through anaconda or pip.\n","\n","`conda install -c conda-forge scrapy`\n","\n","or\n","\n","`pip install Scrapy`\n","\n","### Creating new Scrapy Project\n","\n","Open a terminal (mac/linux) or command line (windows).\n","\n","`scrapy startproject h8scrapy`\n","\n","Once the project is created, there will be a folder and a configuration file created.\n","\n","This folder is created to collate different components of the crawler that will be created later.\n","\n","Once you enter the project folder, you can see the project structure and supporting files.\n","\n","Let’s have a look at them in detail:\n","\n","| File |\tDescription |\n","|---|---|\n","spiders\t| This directory contains all the spiders in the form of a python class. Whenever Scrapy is requested to run, it will be searched in this folder.\n","items.py\t| Includes container that will be loaded along with scraped data.\n","middleware.py | It contains Spider’s processing mechanism to handle requests and responses.\n","pipeline.py\t| It contains a set of Python classes to process scraped data further.\n","settings.py\t| Any customized settings can be added to this file.\n","\n","### Creating Spider\n","\n","Spider is a class that contains the methodology to scrape and extract the data form the site defined. In other words, it determines how to perform the crawl. \n"," \n","In order to create a Spider, we can use the command below.\n","\n","`scrapy genspider <spidername> <your-link-here>`\n","\n","For spidername, you can give any name for your spider and for the link, you can give the URL of the site or domain that you are going to scrape data from. In this session, we will extract manga info from manganato.com.\n","\n","We will call our spider as reviewspider.\n","\n","`scrapy genspider reviewspider https://manganato.com/genre-all`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kYwq1v4AZawx"},"source":["### Scrapy Shell\n","\n","Scrapy shell is an interactive shell similar to a python shell in which you can try and debug your code for data scraping. Using this shell, you can test out your XPath and CSS expressions and verify the data that they extract without even having to run your spider. Therefore, it is a faster and a valuable tool for developing and debugging.\n","Scrapy shell can be launched using the below command\n","\n","`scrapy shell <url>`\n","\n","### Identifying the HTML structure\n","\n","Before we start coding our spider we need to analyze how the web page is structured and identify its patterns. In order to view the HTML structure of the page, we can right-click and go to Inspect, or we can view it through the Browser’s Developer Tools.\n","\n","When we further expand each manga division, we can see there are separate blocks for each component of the manga. Our focus will be on the manga_name, rating and the link.\n","\n","```html\n","<a rel=\"nofollow\" class=\"genres-item-name text-nowrap a-h\" href=\"https://readmanganato.com/manga-il986046\" title=\"Pumpkin Time\">Pumpkin Time</a>\n","```\n"," \n","Here, manga ratings are identified by the class `genres-item-rate`.\n","\n","```html\n","<em class=\"genres-item-rate\">4.6</em>\n","```\n","\n","The review text is identified by the class `genres-item-description`. Next, we will use this information to define our Spider."]},{"cell_type":"markdown","metadata":{"id":"z09eW-24bLWP"},"source":["### Defining Scrapy Parser\n","\n","```py\n","import scrapy\n","\n","\n","class ReviewspiderSpider(scrapy.Spider):\n","    name = 'reviewspider'\n","    allowed_domains = ['https://manganato.com/']\n","    start_urls = ['https://manganato.com/genre-all']\n","\n","    def parse(self, response):\n","        pass\n","\n","```\n","\n","This is the basic template of the Spider and `allowed_domains` and `start_urls` are created based on the link we provided when we created the spider.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vluimF3iS2_8"},"source":["This is the basic template of the Spider and `allowed_domains` and `start_urls` are created based on the link we provided when we created the spider.\n","\n","The logic for extracting our data will be written in the parse function, which will be fired when landing on the page defined by `start_urls`\n","\n","Scrapy allows crawling multiple URLs simultaneously. For this, identify the Base URL and then identify the part of the other URLs that need to join the base URL and append them using urljoin(). However, in this example, we will use only the base URL.\n","\n","Below is the code which is written in the Scrapy Parser to scrape review data.\n","\n","```py\n","def parse(self, response):\n","        manga_name=response.xpath('//a[@class=\"genres-item-name text-nowrap a-h\"]/text()').extract()\n","        viewed=response.xpath('//span[@class=\"genres-item-view\"]/text()').extract()\n","        rating=response.xpath('//em[@class=\"genres-item-rate\"]/text()').extract()\n","        link=response.xpath('//a[@class=\"genres-item-name text-nowrap a-h\"]/@href').extract()\n","\n","        for item in zip(manga_name, viewed, rating, link):\n","\n","            scraped_data = {\n","\n","                'MangaName': item[0],\n","                'Viewed': item[1],\n","                'Rating': item[2],\n","                'Link': item[3]\n","\n","            }\n","\n","            # yield or give the scraped info to scrapy\n","\n","            yield scraped_data\n","```\n","\n","Scrapy comes with its own mechanism called Selectors to extract data. These Selectors use XPath and CSS expressions to select different elements in the HTML documents. In code above uses XPath as the Selector.\n","\n","```py\n","viewed=response.xpath('//span[@class=\"genres-item-view\"]/text()').extract()\n","```\n","\n","In the above code line, Scrapy uses XPath to reach a node in the response and extract its data in the form of a text.\n","\n","```py\n"," for item in zip(manga_name, viewed, rating, link):\n","\n","            scraped_data = {\n","\n","                'MangaName': item[0],\n","                'Viewed': item[1],\n","                'Rating': item[2],\n","                'Link': item[3]\n","\n","            }\n","```\n","\n","In the above code, we are adding each item to the Python dictionary.\n","\n","`yield scraped_data`\n"," \n","The yield statement returns the scraped data for Scrapy to process and store."]},{"cell_type":"markdown","metadata":{"id":"xsR8YiD8UBJW"},"source":["### Running Spider\n","\n","`scrapy runspider h8scrapy/spiders/reviewspider.py -o scraped_data.csv`\n","\n","The runspider command takes the reviewspider.py as the input file and produces the CVS file scraped_data.cvs, which has the collected results.\n","\n","### Scrapy Feed Exports\n","\n","Scrapy provides the Feed Export option to store the extracted data in different formats or serialization methods. It supports formats such as CVS, XML, and JSON.\n","\n","For example, if you want your output in CVS format, got to settings.py file and type in the below lines.\n","\n","```py\n","FEED_FORMAT=\"csv\"\n","\n","FEED_URI=\"scraped_data.csv\"\n","```\n","\n","Save this file and rerun the spider. Then, you can see the CVS file formed under your project directory.\n","\n","If you want a timestamp or name of the spider along with your file name you can use %(time)s or %(name)s to you FEED_URI, For example:\n","\n","`'FEED_URI': \"scraped_data_%(time)s.json\"`"]},{"cell_type":"markdown","metadata":{"id":"bp8nYMWRXyoJ"},"source":["### Handling Multiple Pages\n","\n","This part is about getting additional elements to put in the `start_urls` list. We are finding out how to go to the next page so we can get additional urls to put in `start_urls`.\n","\n","The second start url is: `https://manganato.com/genre-all/2`\n","\n","The code below will be used in the code for the spider, all it does is make a list of start_urls.\n","\n","```py\n","for i in range(2, 1000):\n","  start_urls.append('https://manganato.com/genre-all/'+str(i)+'')\n","```"]},{"cell_type":"markdown","metadata":{"id":"kxMnnCtH94rN"},"source":["### Handling Individual Page\n","\n","The best way to learn how to extract data with Scrapy is using the Scrapy shell. We will use XPaths which can be used to select elements from HTML documents.\n","\n","The first thing we will try and get the xpaths for are the individual manga links. First we do inspect to see roughly where the manga are in the HTML.\n","\n","`<a class=\"genres-item-name text-nowrap a-h\" href=\"https://manganato.com/manga-kc961759\" title=\"Sensou Gekijou\">Sensou Gekijou</a>`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iT5fVP-X-s5k"},"source":["`scrapy shell \"https://manganato.com/genre-all\"`\n","\n","`response.xpath('//a[@class=\"genres-item-name text-nowrap a-h\"]/@href').extract()`\n","\n","```py\n","  for href in response.xpath('//a[@class=\"genres-item-name text-nowrap a-h\"]/@href'):\n","    url  = href.extract()\n","```\n","\n","### Inspect Individual Manga\n","\n","Next we go to individual manga page `https://manganato.com/manga-kc961759`. Using the same process we extract everything we want.\n","\n","### Scrapy Items\n","\n","The main goal in scraping is to extract structured data from unstructured sources, typically, web pages. Scrapy spiders can return the extracted data as Python dicts. While convenient and familiar, Python dicts lack structure: it is easy to make a typo in a field name or return inconsistent data, especially in a larger project with many spiders.\n","\n","```py\n","class H8ScrapyItem(scrapy.Item):\n","    mangaName = scrapy.Field()\n","    author = scrapy.Field()\n","    genre = scrapy.Field()\n","    view = scrapy.Field()\n","    rating = scrapy.Field()\n","    desc = scrapy.Field()\n","\n","```\n","\n","### Final Spider\n","\n","```py\n","import scrapy\n","from h8scrapy.items import H8ScrapyItem\n","\n","\n","class ReviewspiderSpider(scrapy.Spider):\n","    name = 'reviewspiderMultiple'\n","    start_urls = ['https://manganato.com/genre-all']\n","\n","    for i in range(2, 100):\n","        start_urls.append('https://manganato.com/genre-all/'+str(i)+'')\n","    \n","    def parse(self, response):\n","        for href in response.xpath('//a[@class=\"genres-item-name text-nowrap a-h\"]/@href'):\n","            url  = href.extract() \n","            yield scrapy.Request(url, callback=self.parse_dir_contents)\n","\n","    def parse_dir_contents(self, response):\n","        item = H8ScrapyItem()\n","\n","        item['mangaName'] = response.xpath('//div[@class=\"story-info-right\"]/h1/text()').extract()\n","        item['author'] = response.xpath('//td[@class=\"table-value\"]/a/text()').extract()[0]\n","        item['genre'] = response.xpath('//td[@class=\"table-value\"]/a/text()').extract()[1:]\n","        item['view'] = response.xpath('//span[@class=\"stre-value\"]/text()').extract()[1]\n","        item['rating'] = response.xpath('//em[@property=\"v:average\"]/text()').extract()\n","        item['desc'] = response.xpath('//div[@class=\"panel-story-info-description\"]/text()').extract()[1].strip()\n","\n","        yield item\n","```\n","\n","Run with `scrapy crawl reviewspiderMultiple`\n"]}]}