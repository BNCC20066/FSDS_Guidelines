{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"nav_menu":{"height":"264px","width":"369px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false},"colab":{"name":"d4am.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"F6ZYMG5zc4Ya"},"source":["# Week 1: Day 4 AM // Loading and Preprocessing Data with TensorFlow"]},{"cell_type":"markdown","metadata":{"id":"446wX8mgc1sP"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"gxLPIBX6c1sP"},"source":["First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."]},{"cell_type":"code","metadata":{"id":"ZmOB4Wl4c1sQ"},"source":["# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","# Where to save the figures\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"data\"\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n","os.makedirs(IMAGES_PATH, exist_ok=True)\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GN3LYqPTc1sR"},"source":["## Datasets ( Data API)"]},{"cell_type":"markdown","metadata":{"id":"VH6IhAg5Hcl3"},"source":["We can use Data API in Tensorflow to do data transformation. Dataset can be created from Tensor variable."]},{"cell_type":"code","metadata":{"id":"wOL-r51kc1sR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622694863392,"user_tz":-420,"elapsed":444,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"bd82efcd-0448-4cd8-c923-305a05206d99"},"source":["X = tf.range(10)\n","dataset = tf.data.Dataset.from_tensor_slices(X)\n","dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<TensorSliceDataset shapes: (), types: tf.int32>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"-G1yDYmPc1sS"},"source":["Equivalently:"]},{"cell_type":"code","metadata":{"id":"maSZi7SMc1sS"},"source":["dataset = tf.data.Dataset.range(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z9WIjaeqc1sS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622694870905,"user_tz":-420,"elapsed":393,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"32b7e43c-f47f-442b-9d24-74cca3f92d63"},"source":["for item in dataset:\n","    print(item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(0, shape=(), dtype=int64)\n","tf.Tensor(1, shape=(), dtype=int64)\n","tf.Tensor(2, shape=(), dtype=int64)\n","tf.Tensor(3, shape=(), dtype=int64)\n","tf.Tensor(4, shape=(), dtype=int64)\n","tf.Tensor(5, shape=(), dtype=int64)\n","tf.Tensor(6, shape=(), dtype=int64)\n","tf.Tensor(7, shape=(), dtype=int64)\n","tf.Tensor(8, shape=(), dtype=int64)\n","tf.Tensor(9, shape=(), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l15ODxefd2V8"},"source":["### Chaining Transformations\n","\n","Once we have a dataset, we can apply all sorts of transformations to it by\n","calling its transformation methods."]},{"cell_type":"code","metadata":{"tags":["raises-exception"],"id":"DtX1caOic1sT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622694873711,"user_tz":-420,"elapsed":3,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"9bb6e663-3ef0-4c1c-b9c7-bd7c62b68350"},"source":["dataset = dataset.repeat(3).batch(7)\n","for item in dataset:\n","    print(item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n","tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n","tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n","tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n","tf.Tensor([8 9], shape=(2,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yno5p_6weKIS"},"source":["We can also transform the items by calling the map() method."]},{"cell_type":"code","metadata":{"id":"mCGubFwnc1sT"},"source":["dataset = dataset.map(lambda x: x * 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gWvO53K_c1sT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622694878616,"user_tz":-420,"elapsed":4,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"90d6af9c-8e13-4f38-aa80-d6151a997a8f"},"source":["for item in dataset:\n","    print(item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n","tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n","tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n","tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n","tf.Tensor([16 18], shape=(2,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WcQqB8AOc1sU"},"source":["#dataset = dataset.apply(tf.data.experimental.unbatch()) # Now deprecated\n","dataset = dataset.unbatch()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olFFF__ffPcA"},"source":["It is also possible to simply filter the dataset using the filter() method."]},{"cell_type":"code","metadata":{"id":"JRKK384Mc1sU"},"source":["dataset = dataset.filter(lambda x: x < 10)  # keep only items < 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeJkpD7kc1sU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622694881090,"user_tz":-420,"elapsed":2,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"ffe562b1-2ced-4c94-f655-0d61edfba7bd"},"source":["for item in dataset.take(3):\n","    print(item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(0, shape=(), dtype=int64)\n","tf.Tensor(2, shape=(), dtype=int64)\n","tf.Tensor(4, shape=(), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NSiwgqi8fW-R"},"source":["We can shuffle the dataset by *shuffle()* function."]},{"cell_type":"code","metadata":{"id":"bIA3LIvhc1sU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622694887236,"user_tz":-420,"elapsed":930,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"5f91c83c-85ed-4078-9a88-501675b5ce07"},"source":["tf.random.set_seed(42)\n","\n","dataset = tf.data.Dataset.range(10).repeat(3)\n","dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n","for item in dataset:\n","    print(item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\n","tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\n","tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\n","tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n","tf.Tensor([6 9], shape=(2,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DKP3gXYkgJDa"},"source":["## California Dataset Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"oxFx0C0rH6S_"},"source":["Now, let's try to use Tensorflow data API for modeling. We will use California Housing dataset."]},{"cell_type":"markdown","metadata":{"id":"hYq_8qtbc1sV"},"source":["### Split the California dataset to multiple CSV files"]},{"cell_type":"markdown","metadata":{"id":"LgH5GH_sc1sV"},"source":["Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it with standard normalizer."]},{"cell_type":"code","metadata":{"id":"LM8udxmkc1sV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622694903549,"user_tz":-420,"elapsed":2516,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"5a40c013-ef61-4b4e-c061-6560894c285f"},"source":["from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","housing = fetch_california_housing()\n","X_train_full, X_test, y_train_full, y_test = train_test_split(\n","    housing.data, housing.target.reshape(-1, 1), random_state=42)\n","X_train, X_valid, y_train, y_valid = train_test_split(\n","    X_train_full, y_train_full, random_state=42)\n","\n","scaler = StandardScaler()\n","scaler.fit(X_train)\n","X_mean = scaler.mean_\n","X_std = scaler.scale_"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"nx7_GX4tc1sV"},"source":["For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. To demonstrate this, let's start by splitting the housing dataset and save it to 20 CSV files."]},{"cell_type":"code","metadata":{"id":"NTTZqcc9c1sW"},"source":["def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n","    housing_dir = os.path.join(\"datasets\", \"housing\")\n","    os.makedirs(housing_dir, exist_ok=True)\n","    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n","\n","    filepaths = []\n","    m = len(data)\n","    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n","        part_csv = path_format.format(name_prefix, file_idx)\n","        filepaths.append(part_csv)\n","        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n","            if header is not None:\n","                f.write(header)\n","                f.write(\"\\n\")\n","            for row_idx in row_indices:\n","                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n","                f.write(\"\\n\")\n","    return filepaths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hWwUkbDVc1sW"},"source":["train_data = np.c_[X_train, y_train]\n","valid_data = np.c_[X_valid, y_valid]\n","test_data = np.c_[X_test, y_test]\n","header_cols = housing.feature_names + [\"MedianHouseValue\"]\n","header = \",\".join(header_cols)\n","\n","train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n","valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n","test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"atdAIZ7_c1sW"},"source":["Okay, now let's take a peek at the first few lines of one of these CSV files:"]},{"cell_type":"code","metadata":{"id":"Wu7rZtbic1sW","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1622695149807,"user_tz":-420,"elapsed":321,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"2de89444-e283-4b7f-e8d0-f208434b01dd"},"source":["import pandas as pd\n","\n","pd.read_csv(train_filepaths[0]).head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MedInc</th>\n","      <th>HouseAge</th>\n","      <th>AveRooms</th>\n","      <th>AveBedrms</th>\n","      <th>Population</th>\n","      <th>AveOccup</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","      <th>MedianHouseValue</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.5214</td>\n","      <td>15.0</td>\n","      <td>3.049945</td>\n","      <td>1.106548</td>\n","      <td>1447.0</td>\n","      <td>1.605993</td>\n","      <td>37.63</td>\n","      <td>-122.43</td>\n","      <td>1.442</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5.3275</td>\n","      <td>5.0</td>\n","      <td>6.490060</td>\n","      <td>0.991054</td>\n","      <td>3464.0</td>\n","      <td>3.443340</td>\n","      <td>33.69</td>\n","      <td>-117.39</td>\n","      <td>1.687</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3.1000</td>\n","      <td>29.0</td>\n","      <td>7.542373</td>\n","      <td>1.591525</td>\n","      <td>1328.0</td>\n","      <td>2.250847</td>\n","      <td>38.44</td>\n","      <td>-122.98</td>\n","      <td>1.621</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7.1736</td>\n","      <td>12.0</td>\n","      <td>6.289003</td>\n","      <td>0.997442</td>\n","      <td>1054.0</td>\n","      <td>2.695652</td>\n","      <td>33.55</td>\n","      <td>-117.70</td>\n","      <td>2.621</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.0549</td>\n","      <td>13.0</td>\n","      <td>5.312457</td>\n","      <td>1.085092</td>\n","      <td>3297.0</td>\n","      <td>2.244384</td>\n","      <td>33.93</td>\n","      <td>-116.93</td>\n","      <td>0.956</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   MedInc  HouseAge  AveRooms  ...  Latitude  Longitude  MedianHouseValue\n","0  3.5214      15.0  3.049945  ...     37.63    -122.43             1.442\n","1  5.3275       5.0  6.490060  ...     33.69    -117.39             1.687\n","2  3.1000      29.0  7.542373  ...     38.44    -122.98             1.621\n","3  7.1736      12.0  6.289003  ...     33.55    -117.70             2.621\n","4  2.0549      13.0  5.312457  ...     33.93    -116.93             0.956\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"yRB97vRmKwr1"},"source":["The size of one csv file in bytes is"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVvrfsvbKwT1","executionInfo":{"status":"ok","timestamp":1622695189277,"user_tz":-420,"elapsed":344,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"a86c8ceb-5d2f-4236-dec1-32dd434748a2"},"source":["os.path.getsize(train_filepaths[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["53289"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"wEh9pG7tOQe6"},"source":["For comparison, let's write all the data into a single CSV and see the size."]},{"cell_type":"code","metadata":{"id":"OHAM7xAxMlQt"},"source":["import re\n","train_folder=('/').join(re.search('(.*)/(.*)/(.*)',train_filepaths[0], re.IGNORECASE).group(1,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"nm80IeyLL8DJ","executionInfo":{"status":"ok","timestamp":1622695978354,"user_tz":-420,"elapsed":311,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"826103b5-4738-47ab-c0fe-56d08e668df5"},"source":["print(train_filepaths[0])\n","train_folder"],"execution_count":null,"outputs":[{"output_type":"stream","text":["datasets/housing/my_train_00.csv\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'datasets/housing'"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"__pNBzqzLcoA"},"source":["full_csv = train_folder+'/full_train.csv'\n","with open(full_csv, \"wt\", encoding=\"utf-8\") as f:\n","    if header is not None:\n","        f.write(header)\n","        f.write(\"\\n\")\n","    for row_idx in range(train_data.shape[0]):\n","        f.write(\",\".join([repr(col) for col in train_data[row_idx]]))\n","        f.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JpPbpT3OLm7","executionInfo":{"status":"ok","timestamp":1622696060491,"user_tz":-420,"elapsed":315,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"e50562bc-8890-477c-c2aa-a991b892194b"},"source":["os.path.getsize(full_csv)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1066033"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"QBCU5clSOWns"},"source":["For current data the size of all the training data is not that big, around 1 MB. However, in real world data, especially images, it is common for one dataset to have size in GBs, so knowing this splitting and reading file in parts are pretty useful."]},{"cell_type":"markdown","metadata":{"id":"HXES29Ucc1sX"},"source":["Or in text mode:"]},{"cell_type":"code","metadata":{"id":"tSeSW06Xc1sX","outputId":"ced35d4f-a4ef-4c61-d2e0-4f0d7a92f185"},"source":["with open(train_filepaths[0]) as f:\n","    for i in range(5):\n","        print(f.readline(), end=\"\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n","3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n","5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n","3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n","7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KbxmRSNmc1sX","outputId":"2d3d9820-874d-4668-c037-83ebdf2f51a8"},"source":["train_filepaths"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['datasets/housing/my_train_00.csv',\n"," 'datasets/housing/my_train_01.csv',\n"," 'datasets/housing/my_train_02.csv',\n"," 'datasets/housing/my_train_03.csv',\n"," 'datasets/housing/my_train_04.csv',\n"," 'datasets/housing/my_train_05.csv',\n"," 'datasets/housing/my_train_06.csv',\n"," 'datasets/housing/my_train_07.csv',\n"," 'datasets/housing/my_train_08.csv',\n"," 'datasets/housing/my_train_09.csv',\n"," 'datasets/housing/my_train_10.csv',\n"," 'datasets/housing/my_train_11.csv',\n"," 'datasets/housing/my_train_12.csv',\n"," 'datasets/housing/my_train_13.csv',\n"," 'datasets/housing/my_train_14.csv',\n"," 'datasets/housing/my_train_15.csv',\n"," 'datasets/housing/my_train_16.csv',\n"," 'datasets/housing/my_train_17.csv',\n"," 'datasets/housing/my_train_18.csv',\n"," 'datasets/housing/my_train_19.csv']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"KxjUrjQDc1sX"},"source":["### Building an Input Pipeline"]},{"cell_type":"code","metadata":{"id":"GjpKfPFpc1sX"},"source":["filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T20tbaPzc1sY","outputId":"4f5e35b5-c630-462a-e548-beea290e1ef7"},"source":["for filepath in filepath_dataset:\n","    print(filepath)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y5HrepJDc1sY"},"source":["n_readers = 5\n","dataset = filepath_dataset.interleave(\n","    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n","    cycle_length=n_readers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rke4aPic1sY","outputId":"63c3846c-e5c3-4d50-9c5f-bc880ff3f364"},"source":["for line in dataset.take(5):\n","    print(line.numpy())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\n","b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\n","b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n","b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n","b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wSx8257Cc1sY"},"source":["Notice that field 4 is interpreted as a string. We define default value to fill in the missing fields. We want to fill the first field with 0, second filed with *np.nan*, third field with *nan* from tensorflow, fourth field with \"Hello\"."]},{"cell_type":"code","metadata":{"id":"DDYbwaaZc1sZ","outputId":"cc7b2d66-e11c-477a-fbd6-e3d3a045b287"},"source":["record_defaults=[0, np.nan, tf.constant(np.nan, dtype=tf.float64), \"Hello\", tf.constant([])]\n","parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n","parsed_fields"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n"," <tf.Tensor: shape=(), dtype=float64, numpy=3.0>,\n"," <tf.Tensor: shape=(), dtype=string, numpy=b'4'>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"Lt_9KcDac1sZ"},"source":["Notice that all missing fields are replaced with their default value, when provided:"]},{"cell_type":"code","metadata":{"id":"AfDDd4lWc1sZ","outputId":"e911b2ba-a969-427f-f08d-2ad736b08173"},"source":["parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\n","parsed_fields"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n"," <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n"," <tf.Tensor: shape=(), dtype=string, numpy=b'Hello'>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"AxSMU1lIc1sa"},"source":["The 5th field is compulsory (since we provided `tf.constant([])` as the \"default value\"), so we get an exception if we do not provide it:"]},{"cell_type":"code","metadata":{"id":"60kxdufZc1sa","outputId":"5373085c-ed44-4d57-8de3-fb29b3116631"},"source":["try:\n","    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n","except tf.errors.InvalidArgumentError as ex:\n","    print(ex)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"02zqwQO_c1sa"},"source":["The number of fields should match exactly the number of fields in the `record_defaults`:"]},{"cell_type":"code","metadata":{"id":"AluYAhZ8c1sa","outputId":"c9e277b2-c9e5-4ce1-9faf-637130cb082b"},"source":["try:\n","    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n","except tf.errors.InvalidArgumentError as ex:\n","    print(ex)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZPK-gyIoc1sb"},"source":["n_inputs = 8 # X_train.shape[-1]\n","\n","@tf.function\n","def preprocess(line):\n","    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n","    fields = tf.io.decode_csv(line, record_defaults=defs)\n","    x = tf.stack(fields[:-1])\n","    y = tf.stack(fields[-1:])\n","    return (x - X_mean) / X_std, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wX6rnNJcc1sb","outputId":"9dc04100-e6bd-4f13-e10a-4252f1954916"},"source":["preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n"," array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n","        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n"," <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"cSiFvNO9c1sb"},"source":["def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n","                       n_read_threads=None, shuffle_buffer_size=10000,\n","                       n_parse_threads=5, batch_size=32):\n","    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n","    dataset = dataset.interleave(\n","        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n","        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n","    dataset = dataset.shuffle(shuffle_buffer_size)\n","    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n","    dataset = dataset.batch(batch_size)\n","    return dataset.prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMko0HRlc1sb","outputId":"83f31379-a86d-4258-e786-32543ea501ed"},"source":["tf.random.set_seed(42)\n","\n","train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n","for X_batch, y_batch in train_set.take(2):\n","    print(\"X =\", X_batch)\n","    print(\"y =\", y_batch)\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X = tf.Tensor(\n","[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n","   1.2525111  -1.3671792 ]\n"," [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n","   0.7231292  -1.0023477 ]\n"," [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n","  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n","y = tf.Tensor(\n","[[1.752]\n"," [1.313]\n"," [1.535]], shape=(3, 1), dtype=float32)\n","\n","X = tf.Tensor(\n","[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n","   0.9807942  -0.67250353]\n"," [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n","   1.107282   -0.8674123 ]\n"," [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n","  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n","y = tf.Tensor(\n","[[0.919]\n"," [1.028]\n"," [2.182]], shape=(3, 1), dtype=float32)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vnZify0ShMAz"},"source":["We can then set initialize *csv_reader_dataset* for train set, validation set, and test set, that each read their own .csvs. This way, we can process large dataset with limited memory."]},{"cell_type":"code","metadata":{"id":"kL6Dlvj-c1sc"},"source":["train_set = csv_reader_dataset(train_filepaths, repeat=None)\n","valid_set = csv_reader_dataset(valid_filepaths)\n","test_set = csv_reader_dataset(test_filepaths)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cj89HlElhhkb"},"source":["Let's initialize the model, optimizer, and train the model to validate whether our code works."]},{"cell_type":"code","metadata":{"id":"TuNefSkCc1sc"},"source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOk5qINEc1sc"},"source":["model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzOti7AIc1sc","outputId":"6cbd1664-605c-4e8e-cad4-c248802a8763"},"source":["batch_size = 32\n","model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n","          validation_data=valid_set)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","362/362 [==============================] - 1s 3ms/step - loss: 2.0914 - val_loss: 21.5124\n","Epoch 2/10\n","362/362 [==============================] - 0s 1ms/step - loss: 0.8428 - val_loss: 0.6648\n","Epoch 3/10\n","362/362 [==============================] - 0s 1ms/step - loss: 0.6329 - val_loss: 0.6196\n","Epoch 4/10\n","362/362 [==============================] - 0s 1ms/step - loss: 0.5922 - val_loss: 0.5669\n","Epoch 5/10\n","362/362 [==============================] - 0s 1ms/step - loss: 0.5622 - val_loss: 0.5402\n","Epoch 6/10\n","362/362 [==============================] - 0s 1ms/step - loss: 0.5698 - val_loss: 0.5209\n","Epoch 7/10\n","362/362 [==============================] - 0s 1ms/step - loss: 0.5195 - val_loss: 0.6130\n","Epoch 8/10\n","362/362 [==============================] - 0s 1ms/step - loss: 0.5155 - val_loss: 0.4818\n","Epoch 9/10\n","362/362 [==============================] - 0s 1ms/step - loss: 0.4965 - val_loss: 0.4904\n","Epoch 10/10\n","362/362 [==============================] - 0s 1ms/step - loss: 0.4925 - val_loss: 0.4585\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fd68051ec50>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"YXsIzq0hc1sd","outputId":"f4719816-1722-4b37-c5e0-5951c3732760"},"source":["model.evaluate(test_set, steps=len(X_test) // batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["161/161 [==============================] - 0s 589us/step - loss: 0.4788\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.4787752032279968"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"_R2yItd3c1sd","outputId":"ba2e924e-a003-43f6-fb54-31af8b39a978"},"source":["new_set = test_set.map(lambda X, y: X) # we could instead just pass test_set, Keras would ignore the labels\n","X_new = X_test\n","model.predict(new_set, steps=len(X_new) // batch_size)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2.3576407],\n","       [2.255291 ],\n","       [1.4437605],\n","       ...,\n","       [0.5654393],\n","       [3.9442453],\n","       [1.0232248]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"wKpznnk1jTZo"},"source":["To train the model directly with Tensorflow interface, we can use the following code."]},{"cell_type":"code","metadata":{"id":"__rifb65c1sd","outputId":"a11c3c9e-e611-4878-d70b-cf9cdba4bb76"},"source":["optimizer = keras.optimizers.Nadam(lr=0.01)\n","loss_fn = keras.losses.mean_squared_error\n","\n","n_epochs = 5\n","batch_size = 32\n","n_steps_per_epoch = len(X_train) // batch_size\n","total_steps = n_epochs * n_steps_per_epoch\n","global_step = 0\n","for X_batch, y_batch in train_set.take(total_steps):\n","    global_step += 1\n","    print(\"\\rGlobal step {}/{}\".format(global_step, total_steps), end=\"\")\n","    with tf.GradientTape() as tape:\n","        y_pred = model(X_batch)\n","        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","        loss = tf.add_n([main_loss] + model.losses)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Global step 1810/1810"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fjSHxiIbkVk2"},"source":["We can also wrap the training routine in a tensorflow function."]},{"cell_type":"code","metadata":{"id":"wpxTltDFc1sd"},"source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XNzI2v1Pl8b3"},"source":["As a reminder, '@' serve as symbol to show decorator.\n","\n","@tf.function\n","\n","def train():\n","\n","is equivalent to\n","\n","def train():\n","\n","train = tf.function(train)"]},{"cell_type":"code","metadata":{"id":"MGZ3sD0Sc1se"},"source":["optimizer = keras.optimizers.Nadam(lr=0.01)\n","loss_fn = keras.losses.mean_squared_error\n","\n","@tf.function\n","def train(model, n_epochs, batch_size=32,\n","          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n","    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n","                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n","                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n","    for X_batch, y_batch in train_set:\n","        with tf.GradientTape() as tape:\n","            y_pred = model(X_batch)\n","            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","            loss = tf.add_n([main_loss] + model.losses)\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","train(model, 5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MF9muwKbm2XM"},"source":["On following code, we update the training by mini batches."]},{"cell_type":"code","metadata":{"id":"jv8938tXc1se"},"source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LpjGEn5Wc1se","outputId":"f20c1639-35e3-4952-ef48-9b331e2be45a"},"source":["optimizer = keras.optimizers.Nadam(lr=0.01)\n","loss_fn = keras.losses.mean_squared_error\n","\n","@tf.function\n","def train(model, n_epochs, batch_size=32,\n","          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n","    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n","                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n","                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n","    n_steps_per_epoch = len(X_train) // batch_size\n","    total_steps = n_epochs * n_steps_per_epoch\n","    global_step = 0\n","    for X_batch, y_batch in train_set.take(total_steps):\n","        global_step += 1\n","        if tf.equal(global_step % 100, 0):\n","            tf.print(\"\\rGlobal step\", global_step, \"/\", total_steps)\n","        with tf.GradientTape() as tape:\n","            y_pred = model(X_batch)\n","            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","            loss = tf.add_n([main_loss] + model.losses)\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","train(model, 5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Global step 100 / 1810\n","Global step 200 / 1810\n","Global step 300 / 1810\n","Global step 400 / 1810\n","Global step 500 / 1810\n","Global step 600 / 1810\n","Global step 700 / 1810\n","Global step 800 / 1810\n","Global step 900 / 1810\n","Global step 1000 / 1810\n","Global step 1100 / 1810\n","Global step 1200 / 1810\n","Global step 1300 / 1810\n","Global step 1400 / 1810\n","Global step 1500 / 1810\n","Global step 1600 / 1810\n","Global step 1700 / 1810\n","Global step 1800 / 1810\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o__9P9sNc1se"},"source":["Here is a short description of each method in the `Dataset` class:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"4RIQ9mN9c1sf","outputId":"53bedc37-cbe4-4ba6-f910-a8128aee3712"},"source":["for m in dir(tf.data.Dataset):\n","    if not (m.startswith(\"_\") or m.endswith(\"_\")):\n","        func = getattr(tf.data.Dataset, m)\n","        if hasattr(func, \"__doc__\"):\n","            print(\"● {:21s}{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["● apply()              Applies a transformation function to this dataset.\n","● as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n","● batch()              Combines consecutive elements of this dataset into batches.\n","● cache()              Caches the elements in this dataset.\n","● cardinality()        Returns the cardinality of the dataset, if known.\n","● concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n","● element_spec()       The type specification of an element of this dataset.\n","● enumerate()          Enumerates the elements of this dataset.\n","● filter()             Filters this dataset according to `predicate`.\n","● flat_map()           Maps `map_func` across this dataset and flattens the result.\n","● from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n","● from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n","● from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n","● interleave()         Maps `map_func` across this dataset, and interleaves the results.\n","● list_files()         A dataset of all files matching one or more glob patterns.\n","● map()                Maps `map_func` across the elements of this dataset.\n","● options()            Returns the options for this dataset and its inputs.\n","● padded_batch()       Combines consecutive elements of this dataset into padded batches.\n","● prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n","● range()              Creates a `Dataset` of a step-separated range of values.\n","● reduce()             Reduces the input dataset to a single element.\n","● repeat()             Repeats this dataset so each original value is seen `count` times.\n","● shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n","● shuffle()            Randomly shuffles the elements of this dataset.\n","● skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n","● take()               Creates a `Dataset` with at most `count` elements from this dataset.\n","● unbatch()            Splits elements of a dataset into multiple elements.\n","● window()             Combines (nests of) input elements into a dataset of (nests of) windows.\n","● with_options()       Returns a new `tf.data.Dataset` with the given options set.\n","● zip()                Creates a `Dataset` by zipping together the given datasets.\n"],"name":"stdout"}]}]}