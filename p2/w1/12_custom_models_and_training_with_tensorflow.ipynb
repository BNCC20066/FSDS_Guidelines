{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"12_custom_models_and_training_with_tensorflow.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"BjQ2rWxDNdL9"},"source":["**Chapter 12 – Custom Models and Training with TensorFlow**"]},{"cell_type":"markdown","metadata":{"id":"uY6SPMt1NdL_"},"source":["_This notebook contains all the sample code in chapter 12._"]},{"cell_type":"markdown","metadata":{"id":"3QiiGuHeNdMA"},"source":["<table align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/12_custom_models_and_training_with_tensorflow.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"OIjgREr_NdMA"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"Ap1nvcoyNdMB"},"source":["First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."]},{"cell_type":"code","metadata":{"id":"zuU0M2HlNdMB"},"source":["# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","\n","# TensorFlow ≥2.4 is required in this notebook\n","# Earlier 2.x versions will mostly work the same, but with a few bugs\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.4\"\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","# Where to save the figures\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"deep\"\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n","os.makedirs(IMAGES_PATH, exist_ok=True)\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HqzOZdloNdMC"},"source":["## Tensors and operations"]},{"cell_type":"markdown","metadata":{"id":"BZopopFyNdMC"},"source":["### Tensors"]},{"cell_type":"code","metadata":{"id":"h0Fch2hDNdMC","outputId":"3c27cd14-eb8e-41c6-d81b-f198a5ca869d"},"source":["tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n","array([[1., 2., 3.],\n","       [4., 5., 6.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"AHzX3y4vNdMD","outputId":"c80695c6-1e1c-494e-ed83-efe39b003fbe"},"source":["tf.constant(42) # scalar"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=int32, numpy=42>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"kc3sSJeCNdMD","outputId":"f0c6f275-3e34-4338-e1ae-9342db6780bb"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n","array([[1., 2., 3.],\n","       [4., 5., 6.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"z1C04HZeNdME","outputId":"00d5dd60-40c5-4667-dfbe-669114040688"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([2, 3])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"hU_ihnsYNdME","outputId":"f570901e-550c-453a-cc4c-b1de9616bf11"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tf.float32"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"w5HFw2PxNdME"},"source":["### Indexing"]},{"cell_type":"code","metadata":{"id":"kQUfheEaNdME","outputId":"deeb0fad-0773-4098-97e4-3e9ba98c613b"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n","array([[2., 3.],\n","       [5., 6.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"VJauzC7tNdMF","outputId":"cbf51947-21ee-4b9f-bd97-33b1dcf1f19f"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n","array([[2.],\n","       [5.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"bsJf-6IfNdMF"},"source":["### Ops"]},{"cell_type":"code","metadata":{"id":"hM8JTufUNdMF","outputId":"a603d43d-cf28-46ed-98e4-1b7233ef8040"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n","array([[11., 12., 13.],\n","       [14., 15., 16.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"iF6exLM2NdMF","outputId":"df9b52ea-e31f-401f-b175-94d2b434422a"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n","array([[ 1.,  4.,  9.],\n","       [16., 25., 36.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"ZY65o3vVNdMG","outputId":"1b8393a8-4dc3-428a-9936-d40d4dc109b0"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n","array([[14., 32.],\n","       [32., 77.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"37O9rmSENdNF"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"E1wwbeW-NdNG"},"source":["## 1. to 11.\n","See Appendix A."]},{"cell_type":"markdown","metadata":{"id":"j7CQRpSHNdNG"},"source":["# 12. Implement a custom layer that performs _Layer Normalization_\n","_We will use this type of layer in Chapter 15 when using Recurrent Neural Networks._"]},{"cell_type":"markdown","metadata":{"id":"xMiyNlJYNdNG"},"source":["### a.\n","_Exercise: The `build()` method should define two trainable weights *α* and *β*, both of shape `input_shape[-1:]` and data type `tf.float32`. *α* should be initialized with 1s, and *β* with 0s._"]},{"cell_type":"markdown","metadata":{"id":"pLJzNXqHNdNH"},"source":["Solution: see below."]},{"cell_type":"markdown","metadata":{"id":"DqYWFCpfNdNH"},"source":["### b.\n","_Exercise: The `call()` method should compute the mean_ μ _and standard deviation_ σ _of each instance's features. For this, you can use `tf.nn.moments(inputs, axes=-1, keepdims=True)`, which returns the mean μ and the variance σ<sup>2</sup> of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return *α*⊗(*X* - μ)/(σ + ε) + *β*, where ⊗ represents itemwise multiplication (`*`) and ε is a smoothing term (small constant to avoid division by zero, e.g., 0.001)._"]},{"cell_type":"code","metadata":{"id":"soKd75VPNdNI"},"source":["class LayerNormalization(keras.layers.Layer):\n","    def __init__(self, eps=0.001, **kwargs):\n","        super().__init__(**kwargs)\n","        self.eps = eps\n","\n","    def build(self, batch_input_shape):\n","        self.alpha = self.add_weight(\n","            name=\"alpha\", shape=batch_input_shape[-1:],\n","            initializer=\"ones\")\n","        self.beta = self.add_weight(\n","            name=\"beta\", shape=batch_input_shape[-1:],\n","            initializer=\"zeros\")\n","        super().build(batch_input_shape) # must be at the end\n","\n","    def call(self, X):\n","        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n","        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n","\n","    def compute_output_shape(self, batch_input_shape):\n","        return batch_input_shape\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config, \"eps\": self.eps}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4vKq8FaNdNS"},"source":["Note that making _ε_ a hyperparameter (`eps`) was not compulsory. Also note that it's preferable to compute `tf.sqrt(variance + self.eps)` rather than `tf.sqrt(variance) + self.eps`. Indeed, the derivative of sqrt(z) is undefined when z=0, so training will bomb whenever the variance vector has at least one component equal to 0. Adding _ε_ within the square root guarantees that this will never happen."]},{"cell_type":"markdown","metadata":{"id":"Q8NZN80rNdNT"},"source":["### c.\n","_Exercise: Ensure that your custom layer produces the same (or very nearly the same) output as the `keras.layers.LayerNormalization` layer._"]},{"cell_type":"markdown","metadata":{"id":"og0o92LwNdNU"},"source":["Let's create one instance of each class, apply them to some data (e.g., the training set), and ensure that the difference is negligeable."]},{"cell_type":"code","metadata":{"id":"NiMzrt07NdNU","outputId":"3a182684-0e8f-43d1-bc55-4d7a7455a3ff"},"source":["X = X_train.astype(np.float32)\n","\n","custom_layer_norm = LayerNormalization()\n","keras_layer_norm = keras.layers.LayerNormalization()\n","\n","tf.reduce_mean(keras.losses.mean_absolute_error(\n","    keras_layer_norm(X), custom_layer_norm(X)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=5.6045884e-08>"]},"metadata":{"tags":[]},"execution_count":262}]},{"cell_type":"markdown","metadata":{"id":"RbIwGs0LNdNU"},"source":["Yep, that's close enough. To be extra sure, let's make alpha and beta completely random and compare again:"]},{"cell_type":"code","metadata":{"id":"lnkZubywNdNV","outputId":"577df9ab-35e2-4155-d534-523c0c638f78"},"source":["random_alpha = np.random.rand(X.shape[-1])\n","random_beta = np.random.rand(X.shape[-1])\n","\n","custom_layer_norm.set_weights([random_alpha, random_beta])\n","keras_layer_norm.set_weights([random_alpha, random_beta])\n","\n","tf.reduce_mean(keras.losses.mean_absolute_error(\n","    keras_layer_norm(X), custom_layer_norm(X)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=2.2921004e-08>"]},"metadata":{"tags":[]},"execution_count":263}]},{"cell_type":"markdown","metadata":{"id":"jo-t82acNdNW"},"source":["Still a negligeable difference! Our custom layer works fine."]},{"cell_type":"markdown","metadata":{"id":"y8WxqCGONdNW"},"source":["## 13. Train a model using a custom training loop to tackle the Fashion MNIST dataset\n","_The Fashion MNIST dataset was introduced in Chapter 10._"]},{"cell_type":"markdown","metadata":{"id":"7xu4_2Z4NdNW"},"source":["### a.\n","_Exercise: Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch._"]},{"cell_type":"code","metadata":{"id":"BPmwhUUeNdNX"},"source":["(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n","X_train_full = X_train_full.astype(np.float32) / 255.\n","X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n","y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n","X_test = X_test.astype(np.float32) / 255."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgtKQAPRNdNX"},"source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"efICyNQxNdNZ"},"source":["model = keras.models.Sequential([\n","    keras.layers.Flatten(input_shape=[28, 28]),\n","    keras.layers.Dense(100, activation=\"relu\"),\n","    keras.layers.Dense(10, activation=\"softmax\"),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6j7ca7JNdNZ"},"source":["n_epochs = 5\n","batch_size = 32\n","n_steps = len(X_train) // batch_size\n","optimizer = keras.optimizers.Nadam(lr=0.01)\n","loss_fn = keras.losses.sparse_categorical_crossentropy\n","mean_loss = keras.metrics.Mean()\n","metrics = [keras.metrics.SparseCategoricalAccuracy()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N3oKAthzNdNZ","outputId":"aaf75844-bff8-4820-ef5c-4bbaf59f1a89"},"source":["with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n","    for epoch in epochs:\n","        with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n","            for step in steps:\n","                X_batch, y_batch = random_batch(X_train, y_train)\n","                with tf.GradientTape() as tape:\n","                    y_pred = model(X_batch)\n","                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","                    loss = tf.add_n([main_loss] + model.losses)\n","                gradients = tape.gradient(loss, model.trainable_variables)\n","                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","                for variable in model.variables:\n","                    if variable.constraint is not None:\n","                        variable.assign(variable.constraint(variable))                    \n","                status = OrderedDict()\n","                mean_loss(loss)\n","                status[\"loss\"] = mean_loss.result().numpy()\n","                for metric in metrics:\n","                    metric(y_batch, y_pred)\n","                    status[metric.name] = metric.result().numpy()\n","                steps.set_postfix(status)\n","            y_pred = model(X_valid)\n","            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n","            status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n","                tf.constant(y_valid, dtype=np.float32), y_pred))\n","            steps.set_postfix(status)\n","        for metric in [mean_loss] + metrics:\n","            metric.reset_states()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"901e5649b50840538874aed5bab0d4ed","version_major":2,"version_minor":0},"text/plain":["All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f29690a5ade4bd8a6d164d106ba2d31","version_major":2,"version_minor":0},"text/plain":["Epoch 1/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2ea6579132c48c087c7f59e6309387a","version_major":2,"version_minor":0},"text/plain":["Epoch 2/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94fa1527062c4cf7a277a548bbddc855","version_major":2,"version_minor":0},"text/plain":["Epoch 3/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbcfc9a0c4b64151a18cf27080872dd3","version_major":2,"version_minor":0},"text/plain":["Epoch 4/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc3831d9b98e488c837ba9644bf4b94a","version_major":2,"version_minor":0},"text/plain":["Epoch 5/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"_jr4BXYdNdNa"},"source":["### b.\n","_Exercise: Try using a different optimizer with a different learning rate for the upper layers and the lower layers._"]},{"cell_type":"code","metadata":{"id":"c5RaTVaHNdNa"},"source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UcO8807NdNb"},"source":["lower_layers = keras.models.Sequential([\n","    keras.layers.Flatten(input_shape=[28, 28]),\n","    keras.layers.Dense(100, activation=\"relu\"),\n","])\n","upper_layers = keras.models.Sequential([\n","    keras.layers.Dense(10, activation=\"softmax\"),\n","])\n","model = keras.models.Sequential([\n","    lower_layers, upper_layers\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LjIjN0gNdNb"},"source":["lower_optimizer = keras.optimizers.SGD(lr=1e-4)\n","upper_optimizer = keras.optimizers.Nadam(lr=1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2E8ZGK58NdNb"},"source":["n_epochs = 5\n","batch_size = 32\n","n_steps = len(X_train) // batch_size\n","loss_fn = keras.losses.sparse_categorical_crossentropy\n","mean_loss = keras.metrics.Mean()\n","metrics = [keras.metrics.SparseCategoricalAccuracy()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpBVQ6HyNdNc","outputId":"34b7ebde-2039-4b53-9520-3f621b9fca49"},"source":["with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n","    for epoch in epochs:\n","        with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n","            for step in steps:\n","                X_batch, y_batch = random_batch(X_train, y_train)\n","                with tf.GradientTape(persistent=True) as tape:\n","                    y_pred = model(X_batch)\n","                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","                    loss = tf.add_n([main_loss] + model.losses)\n","                for layers, optimizer in ((lower_layers, lower_optimizer),\n","                                          (upper_layers, upper_optimizer)):\n","                    gradients = tape.gradient(loss, layers.trainable_variables)\n","                    optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n","                del tape\n","                for variable in model.variables:\n","                    if variable.constraint is not None:\n","                        variable.assign(variable.constraint(variable))                    \n","                status = OrderedDict()\n","                mean_loss(loss)\n","                status[\"loss\"] = mean_loss.result().numpy()\n","                for metric in metrics:\n","                    metric(y_batch, y_pred)\n","                    status[metric.name] = metric.result().numpy()\n","                steps.set_postfix(status)\n","            y_pred = model(X_valid)\n","            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n","            status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n","                tf.constant(y_valid, dtype=np.float32), y_pred))\n","            steps.set_postfix(status)\n","        for metric in [mean_loss] + metrics:\n","            metric.reset_states()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e81cf85cf7548748ec760afcbd71aa2","version_major":2,"version_minor":0},"text/plain":["All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"475109b927d044a7bba030f234c67838","version_major":2,"version_minor":0},"text/plain":["Epoch 1/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af0a22afae4f47359f8fdfbac96e38bb","version_major":2,"version_minor":0},"text/plain":["Epoch 2/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5519ec96e42f4281987a84a5434a0734","version_major":2,"version_minor":0},"text/plain":["Epoch 3/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed04f31b3a7d4b3b9a0cd63b644e2ed5","version_major":2,"version_minor":0},"text/plain":["Epoch 4/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bfc4b1b4d40f4003ab8a3140a68ec883","version_major":2,"version_minor":0},"text/plain":["Epoch 5/5:   0%|          | 0/1718 [00:00<?, ?it/s]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"4sox5Vm-NdNc"},"source":[""],"execution_count":null,"outputs":[]}]}